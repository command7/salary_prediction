{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self, train_csv, train_target_csv, cat_features, num_features, target):\n",
    "        self.cat_features = cat_features\n",
    "        self.num_features = num_features\n",
    "        self.target_label = target\n",
    "        self.data_frame = self.read_data(train_csv, output=\"features\")\n",
    "        self.target_data = self.read_data(train_target_csv, output=\"target\")\n",
    "        self.feature_names = list(self.data_frame.columns.values)\n",
    "        self.Xtrain = None\n",
    "        self.ytrain = None\n",
    "        self.Xtest = None\n",
    "        self.ytest = None\n",
    "        self.pre_process()\n",
    "        self.split_data()\n",
    "        \n",
    "    def standardize_column_names(self):\n",
    "        old_column_names = self.feature_names\n",
    "        new_column_names = [column_name.lower() for column_name in old_column_names]\n",
    "        self.feature_names = new_column_names\n",
    "        self.data_frame.columns = self.feature_names\n",
    "\n",
    "    def convert_to_cat_dtype(self):\n",
    "        for feat in self.cat_features:\n",
    "            self.data_frame[feat] = self.data_frame[feat].astype(\"category\")\n",
    "    \n",
    "    def scale_numeric_features(self):\n",
    "        num_data = self.data_frame[self.num_features].values\n",
    "        std_scaler = StandardScaler()\n",
    "        scaled_values = std_scaler.fit_transform(num_data)\n",
    "        num_df = pd.DataFrame(scaled_values, columns=self.num_features)\n",
    "        self.data_frame = pd.concat([num_df, self.data_frame[self.cat_features]], axis=1)\n",
    "        \n",
    "    def encode_cat_features(self):\n",
    "        new_df = self.data_frame[self.num_features].copy()\n",
    "        for cat_feature in self.cat_features:\n",
    "            encoded_output = pd.get_dummies(self.data_frame[cat_feature])\n",
    "            new_df = pd.concat([new_df, encoded_output], axis=1)\n",
    "        self.data_frame = new_df\n",
    "            \n",
    "            \n",
    "    def pre_process(self):\n",
    "        self.standardize_column_names()\n",
    "        self.convert_to_cat_dtype()\n",
    "        self.scale_numeric_features()\n",
    "        self.encode_cat_features()\n",
    "        \n",
    "    def read_data(self, csv, output=\"features\"):\n",
    "        df = pd.read_csv(csv)\n",
    "        if output==\"features\":\n",
    "            return df.iloc[:, 1:]\n",
    "        else:\n",
    "            target = df.loc[:,self.target_label]\n",
    "            return target\n",
    "    \n",
    "    def split_data(self):\n",
    "        consolidated_df = pd.concat([self.data_frame, self.target_data],axis=1)\n",
    "        train, test = train_test_split(consolidated_df, test_size=0.2)\n",
    "        self.Xtrain = train.iloc[:, :-1]\n",
    "        self.ytrain = train.iloc[:,-1]\n",
    "        self.Xtest = test.iloc[:, :-1]\n",
    "        self.ytest = test.iloc[:,-1]\n",
    "\n",
    "\n",
    "class Models:\n",
    "    def __init__(self, data):\n",
    "        self.Xtrain = data.Xtrain\n",
    "        self.ytrain = data.ytrain\n",
    "        self.Xtest = data.Xtest\n",
    "        self.ytest = data.ytest\n",
    "        self.models = []\n",
    "        self.model_info = {}\n",
    "        self.predictions = None\n",
    "        self.best_model = None\n",
    "        \n",
    "    \n",
    "    def add_model(self, model):\n",
    "        self.models.append(model)\n",
    "    \n",
    "    def cross_validate(self, k):\n",
    "        for model in self.models:\n",
    "            cross_evaluation = cross_val_score(model, self.Xtrain, self.ytrain, cv=k, scoring=\"neg_mean_squared_error\")\n",
    "            score = np.mean(np.negative(cross_evaluation))\n",
    "            self.model_info[model] = score\n",
    "        \n",
    "    def select_best_model(self):\n",
    "        self.best_model = min(self.modelInfo, key=self.modelInfo.get)\n",
    "        \n",
    "    def summarize(self):\n",
    "        for model in self.models:\n",
    "            print(model)\n",
    "            print(print(self.model[model]))\n",
    "            print(self.best_model)\n",
    "            \n",
    "    def run(self):\n",
    "        self.cross_validate(5)\n",
    "        self.select_best_model()\n",
    "        self.summarize()\n",
    "        \n",
    "            \n",
    "    \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800000, 94)\n",
      "(200000, 94)\n",
      "(800000,)\n",
      "(200000,)\n"
     ]
    }
   ],
   "source": [
    "dataset_features = \"data/train_features.csv\"\n",
    "dataset_target = \"data/train_salaries.csv\"\n",
    "cat_features = [\"companyid\", \"jobtype\", \"degree\", \"major\", \"industry\"]\n",
    "num_features = [\"milesfrommetropolis\", \"yearsexperience\"]\n",
    "\n",
    "lin_alg = LinearRegression()\n",
    "ridge_reg = Ridge()\n",
    "rfr = RandomForestRegressor()\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = Data(dataset_features, dataset_target,cat_features, num_features, \"salary\")\n",
    "    print(data.Xtrain.shape)\n",
    "    print(data.Xtest.shape)\n",
    "    print(data.ytrain.shape)\n",
    "    print(data.ytest.shape)\n",
    "    models = Models(data)\n",
    "    models.add_model(lin_alg)\n",
    "    models.add_model(ridge_reg)\n",
    "    models.add_model(rfr)\n",
    "    models.add_model(gbr)\n",
    "    models.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
