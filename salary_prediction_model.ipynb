{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self, train_csv, train_target_csv, cat_features, num_features, target, scaled=False):\n",
    "        self.cat_features = cat_features\n",
    "        self.num_features = num_features\n",
    "        self.target_label = target\n",
    "        self.data_frame = self.read_data(train_csv, train_target_csv)\n",
    "        self.scaling = scaled\n",
    "        self.target_data = self.data_frame.salary.copy()\n",
    "        self.Xtrain = None\n",
    "        self.ytrain = None\n",
    "        self.Xtest = None\n",
    "        self.ytest = None\n",
    "        self.pre_process()\n",
    "        self.split_data()\n",
    "\n",
    "    def convert_to_cat_dtype(self):\n",
    "        for feat in self.cat_features:\n",
    "            self.data_frame[feat] = self.data_frame[feat].astype(\"category\")\n",
    "    \n",
    "    def scale_numeric_features(self):\n",
    "        num_data = self.data_frame[self.num_features].values\n",
    "        std_scaler = StandardScaler()\n",
    "        scaled_values = std_scaler.fit_transform(num_data)\n",
    "        num_df = pd.DataFrame(scaled_values, columns=self.num_features)\n",
    "        test= pd.concat([num_df, self.data_frame[self.cat_features]], axis=1)\n",
    "        \n",
    "    def encode_cat_features(self):\n",
    "        new_df = self.data_frame[self.num_features].copy()\n",
    "        encoded_output = pd.get_dummies(self.data_frame[self.cat_features])\n",
    "        new_df = pd.concat([new_df, encoded_output], axis=1)\n",
    "        self.data_frame = new_df\n",
    "        \n",
    "    def get_target(self):\n",
    "        return self.data_frame.loc[\"salary\"].copy()\n",
    "            \n",
    "    def clean_data(self, df):\n",
    "        total_df = df.copy()\n",
    "        total_df.drop_duplicates(inplace=True)\n",
    "        clean_df = total_df[total_df.salary >0].copy()\n",
    "        return clean_df\n",
    "\n",
    "    def pre_process(self):\n",
    "        self.convert_to_cat_dtype()\n",
    "        if(self.scaling == True):\n",
    "            self.scale_numeric_features()\n",
    "        self.encode_cat_features()\n",
    "        \n",
    "    def read_data(self, csv1, csv2):\n",
    "        df = pd.read_csv(csv1)\n",
    "        df2 = pd.read_csv(csv2)\n",
    "        total_df = pd.merge(df, df2, how=\"inner\", on=\"jobId\")\n",
    "        clean_df = self.clean_data(total_df)\n",
    "        return clean_df\n",
    "    \n",
    "    def split_data(self):\n",
    "        consolidated_df = pd.concat([self.data_frame, self.target_data],axis=1)\n",
    "        train, test = train_test_split(consolidated_df, test_size=0.2, random_state = 42)\n",
    "        self.Xtrain = train.iloc[:, :-1]\n",
    "        self.ytrain = train.loc[:,\"salary\"]#.values.reshape(-1,1)#, columns=[\"salary\"])\n",
    "        self.Xtest = test.iloc[:, :-1]\n",
    "        self.ytest = test.loc[:,\"salary\"]#.values.reshape(-1,1), columns=[\"salary\"])\n",
    "\n",
    "\n",
    "class Models:\n",
    "    def __init__(self, data, scaled_data=None):\n",
    "        if scaled_data != None:\n",
    "            self.Xtrain_scaled = scaled_data.Xtrain\n",
    "            self.Xtest_scaled = scaled_data.Xtest\n",
    "        self.Xtrain = data.data_frame\n",
    "        self.ytrain = data.target_data\n",
    "        self.Xtest = data.Xtest\n",
    "        self.ytest = data.ytest\n",
    "        self.models = []\n",
    "        self.mse = {}\n",
    "        self.cross_val_scores = {}\n",
    "        self.predictions = None\n",
    "        self.best_model = None    \n",
    "    \n",
    "    def add_model(self, model):\n",
    "        self.models.append(model)\n",
    "            \n",
    "    def cross_validate(self, k):\n",
    "        for model in self.models:\n",
    "#             model.fit(self.Xtrain, self.ytrain)\n",
    "#             self.mse[model.__class__.__name__] = mean_squared_error(self.ytest, model.predict(self.Xtest))\n",
    "            cross_evaluation = cross_val_score(model, self.Xtrain, self.ytrain, cv=k, scoring=\"neg_mean_squared_error\")\n",
    "            score = np.mean(np.negative(cross_evaluation))\n",
    "            self.cross_val_scores[model.__class__.__name__] = score\n",
    "        \n",
    "    def select_best_model(self):\n",
    "        self.best_model = min(self.cross_val_scores, key=self.cross_val_scores.get)\n",
    "        \n",
    "    def summarize(self):\n",
    "        for model in self.models:\n",
    "            print(model.__class__.__name__)\n",
    "            print(\"Mean Squared Error before cross validation: {}\".format(self.mse[model.__class__.__name__]))\n",
    "            print(\"Mean Squared Error after cross validation: {}\".format(self.cross_val_scores[model.__class__.__name__]))\n",
    "            print(\"\\n\")\n",
    "        print(\"Best Model is: {}\".format(self.best_model))\n",
    "            \n",
    "    def hyper_param_tune(self, model, params):\n",
    "        grid_search = GridSearchCV(model, params, cv=5, scoring = \"neg_mean_squared_error\")\n",
    "        grid_search.fit(self.Xtrain, self.ytrain)\n",
    "        return grid_search      \n",
    "            \n",
    "    def run(self):\n",
    "        self.cross_validate(5)\n",
    "        self.select_best_model()\n",
    "        self.summarize()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_features = \"data/train_features.csv\"\n",
    "dataset_target = \"data/train_salaries.csv\"\n",
    "cat_features = [\"companyId\", \"jobType\", \"degree\", \"major\", \"industry\"]\n",
    "num_features = [\"milesFromMetropolis\", \"yearsExperience\"]\n",
    "\n",
    "lin_alg = LinearRegression()\n",
    "# dec_tree = DecisionTreeRegressor(max_leaf_nodes=6, max_depth, max_features = 40)\n",
    "rfr = RandomForestRegressor(n_estimators=100, max_leaf_nodes=10)\n",
    "gbr = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "\n",
    "param_grid = [{'n_estimators':[3,10,30], 'max_features':[10,30,40,50,60]},\n",
    "             {\"bootstrap\":[False], 'n_estimators':[3,10], 'max_features':[5,20, 50,80]}]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = Data(dataset_features, dataset_target,cat_features, num_features, \"salary\", scaled=False)\n",
    "    models = Models(data)\n",
    "    models.add_model(lin_alg)\n",
    "    models.add_model(ridge_reg)\n",
    "    models.add_model(rfr)\n",
    "    models.add_model(gbr)\n",
    "    models.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'RandomForestRegressor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8c6bdc5f8313>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgbr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-ed30eebec0b6>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_best_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-ed30eebec0b6>\u001b[0m in \u001b[0;36msummarize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mean Squared Error before cross validation: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mean Squared Error after cross validation: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_val_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'RandomForestRegressor'"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(n_estimators=100, max_features=25, max_depth=30, min_samples_split=70)\n",
    "gbr = GradientBoostingRegressor(max_depth=5, n_estimators=150)\n",
    "data_two = Data(dataset_features, dataset_target,cat_features, num_features, \"salary\")\n",
    "models = Models(data_two)\n",
    "# models.add_model(lin_alg)\n",
    "#     models.add_model(ridge_reg)\n",
    "models.add_model(rfr)\n",
    "models.add_model(gbr)\n",
    "models.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366.1981194984496\n"
     ]
    }
   ],
   "source": [
    "print(models.cross_val_scores[\"RandomForestRegressor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
